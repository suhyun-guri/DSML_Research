{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b463c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dropout, InputLayer, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#한글설정\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_dirs = ['/usr/share/fonts/truetype/nanum', ]\n",
    "font_files = fm.findSystemFonts(fontpaths=font_dirs)\n",
    "\n",
    "for font_file in font_files:\n",
    "    fm.fontManager.addfont(font_file)\n",
    "    \n",
    "# 한글 출력을 위해서 폰트 옵션을 설정합니다.\n",
    "# \"axes.unicode_minus\" : 마이너스가 깨질 것을 방지\n",
    "\n",
    "sns.set(font=\"NanumBarunGothic\",\n",
    "        rc={\"axes.unicode_minus\":False},\n",
    "        style='darkgrid')\n",
    "\n",
    "#GPU 사용 설정, -1이면 CPU 사용\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for i in range(len(gpus)):\n",
    "            tf.config.experimental.set_memory_growth(gpus[i], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b20bc",
   "metadata": {},
   "source": [
    "<img src='./data/Method_allfit_Sequential.jpg' width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb946b",
   "metadata": {},
   "source": [
    "# Training - all fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb7c93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random    \n",
    "seed_num = 42\n",
    "random.seed(seed_num)\n",
    "\n",
    "X = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "y = np.load('/project/LSH/y_(7727,1).npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf210557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(gpu_mode=False):\n",
    "    seed_num = 42\n",
    "    tf.random.set_seed(seed_num)\n",
    "    if gpu_mode:\n",
    "        lstm = Sequential()\n",
    "        lstm.add(InputLayer(input_shape=(X.shape[1],X.shape[2])))\n",
    "        lstm.add(CuDNNLSTM(units=128, return_sequences=True))\n",
    "        lstm.add(Activation('hard_sigmoid'))\n",
    "        lstm.add(CuDNNLSTM(units=64, return_sequences=True))\n",
    "        lstm.add(Activation('hard_sigmoid'))\n",
    "        lstm.add(Dropout(0.2))\n",
    "        lstm.add(CuDNNLSTM(units=64, return_sequences=True))\n",
    "        lstm.add(Activation('hard_sigmoid'))\n",
    "        lstm.add(CuDNNLSTM(units=32, return_sequences=False))\n",
    "        lstm.add(Activation('hard_sigmoid'))\n",
    "        lstm.add(Dropout(0.2))\n",
    "        lstm.add(Dense(units=1, activation='sigmoid'))\n",
    "    else:\n",
    "        lstm = Sequential()\n",
    "        lstm.add(InputLayer(input_shape=(x.shape[1],x.shape[2])))\n",
    "        lstm.add(LSTM(units=128, activation='hard_sigmoid', return_sequences=True))\n",
    "        lstm.add(LSTM(units=64, activation='hard_sigmoid', return_sequences=True))\n",
    "        lstm.add(Dropout(0.2))\n",
    "        lstm.add(LSTM(units=64, activation='hard_sigmoid', return_sequences=True))\n",
    "        lstm.add(LSTM(units=32, activation='hard_sigmoid', return_sequences=False))\n",
    "        lstm.add(Dropout(0.2))\n",
    "        lstm.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate = 0.001)\n",
    "    lstm.compile(optimizer=optimizer, loss = \"binary_crossentropy\", metrics=['acc'])\n",
    "    return lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02fdb907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 7s 82ms/step - loss: 0.6458 - acc: 0.6511 - val_loss: 0.6934 - val_acc: 0.4865\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.6740 - acc: 0.6066 - val_loss: 0.7237 - val_acc: 0.4865\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 67ms/step - loss: 0.6651 - acc: 0.6419 - val_loss: 0.7362 - val_acc: 0.4865\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.6369 - acc: 0.6518 - val_loss: 0.6848 - val_acc: 0.4865\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 70ms/step - loss: 0.5325 - acc: 0.7370 - val_loss: 0.5727 - val_acc: 0.7091\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.4406 - acc: 0.8109 - val_loss: 0.5526 - val_acc: 0.7340\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.3932 - acc: 0.8412 - val_loss: 0.5593 - val_acc: 0.7407\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 67ms/step - loss: 0.3640 - acc: 0.8616 - val_loss: 0.5701 - val_acc: 0.7536\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.3388 - acc: 0.8732 - val_loss: 0.5873 - val_acc: 0.7526\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 67ms/step - loss: 0.3074 - acc: 0.8942 - val_loss: 0.5998 - val_acc: 0.7578\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.2760 - acc: 0.9079 - val_loss: 0.6232 - val_acc: 0.7572\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.2646 - acc: 0.9154 - val_loss: 0.6584 - val_acc: 0.7443\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.3008 - acc: 0.8877 - val_loss: 0.6143 - val_acc: 0.7474\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.2566 - acc: 0.9134 - val_loss: 0.6698 - val_acc: 0.7479\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 70ms/step - loss: 0.2514 - acc: 0.9179 - val_loss: 0.6553 - val_acc: 0.7578\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.2676 - acc: 0.9047 - val_loss: 0.7805 - val_acc: 0.7158\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.2777 - acc: 0.8928 - val_loss: 0.6401 - val_acc: 0.7500\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.2239 - acc: 0.9303 - val_loss: 0.6679 - val_acc: 0.7552\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 70ms/step - loss: 0.1861 - acc: 0.9474 - val_loss: 0.7194 - val_acc: 0.7552\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.1727 - acc: 0.9534 - val_loss: 0.7564 - val_acc: 0.7562\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.1593 - acc: 0.9575 - val_loss: 0.8107 - val_acc: 0.7428\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.1479 - acc: 0.9620 - val_loss: 0.8626 - val_acc: 0.7371\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.1367 - acc: 0.9660 - val_loss: 0.8842 - val_acc: 0.7376\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.1311 - acc: 0.9686 - val_loss: 0.8628 - val_acc: 0.7484\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.1325 - acc: 0.9664 - val_loss: 0.8894 - val_acc: 0.7386\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.1313 - acc: 0.9650 - val_loss: 0.9075 - val_acc: 0.7376\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.1396 - acc: 0.9620 - val_loss: 0.8972 - val_acc: 0.7391\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 70ms/step - loss: 0.1465 - acc: 0.9591 - val_loss: 0.8933 - val_acc: 0.7350\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.1714 - acc: 0.9465 - val_loss: 0.8166 - val_acc: 0.7381\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.1762 - acc: 0.9431 - val_loss: 0.8671 - val_acc: 0.7200\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.1346 - acc: 0.9639 - val_loss: 0.8399 - val_acc: 0.7453\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.1138 - acc: 0.9695 - val_loss: 0.8864 - val_acc: 0.7428\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.1051 - acc: 0.9741 - val_loss: 0.9352 - val_acc: 0.7329\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.1023 - acc: 0.9745 - val_loss: 0.9884 - val_acc: 0.7288\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0978 - acc: 0.9764 - val_loss: 1.0107 - val_acc: 0.7283\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0927 - acc: 0.9776 - val_loss: 1.0218 - val_acc: 0.7308\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0895 - acc: 0.9784 - val_loss: 1.0389 - val_acc: 0.7293\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0880 - acc: 0.9798 - val_loss: 1.0444 - val_acc: 0.7277\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0881 - acc: 0.9795 - val_loss: 1.0512 - val_acc: 0.7283\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0859 - acc: 0.9808 - val_loss: 1.0592 - val_acc: 0.7257\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0864 - acc: 0.9793 - val_loss: 1.0456 - val_acc: 0.7308\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0911 - acc: 0.9777 - val_loss: 1.0453 - val_acc: 0.7257\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0867 - acc: 0.9789 - val_loss: 1.0583 - val_acc: 0.7277\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0820 - acc: 0.9805 - val_loss: 1.0895 - val_acc: 0.7246\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 67ms/step - loss: 0.0776 - acc: 0.9836 - val_loss: 1.1174 - val_acc: 0.7215\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0772 - acc: 0.9826 - val_loss: 1.1260 - val_acc: 0.7246\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0751 - acc: 0.9817 - val_loss: 1.1414 - val_acc: 0.7231\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0768 - acc: 0.9821 - val_loss: 1.1484 - val_acc: 0.7195\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0762 - acc: 0.9817 - val_loss: 1.1614 - val_acc: 0.7174\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0758 - acc: 0.9840 - val_loss: 1.1599 - val_acc: 0.7205\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0737 - acc: 0.9845 - val_loss: 1.1762 - val_acc: 0.7184\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0752 - acc: 0.9838 - val_loss: 1.1819 - val_acc: 0.7179\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0745 - acc: 0.9831 - val_loss: 1.1856 - val_acc: 0.7184\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0744 - acc: 0.9827 - val_loss: 1.2035 - val_acc: 0.7133\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0748 - acc: 0.9834 - val_loss: 1.1820 - val_acc: 0.7127\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 70ms/step - loss: 0.0832 - acc: 0.9791 - val_loss: 1.1870 - val_acc: 0.7024\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0746 - acc: 0.9831 - val_loss: 1.1390 - val_acc: 0.7189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0799 - acc: 0.9803 - val_loss: 1.1117 - val_acc: 0.7257\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 68ms/step - loss: 0.0901 - acc: 0.9765 - val_loss: 1.0791 - val_acc: 0.7220\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 69ms/step - loss: 0.0826 - acc: 0.9786 - val_loss: 1.0558 - val_acc: 0.7272\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00060: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2962337bb0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_SAVE_FOLDER_PATH = './models/'\n",
    "filepath = MODEL_SAVE_FOLDER_PATH + 'ALLFIT1_{epoch:02d}-{val_acc:.4f}.hdf5'\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, save_weights_only=False, save_freq='epoch')\n",
    "\n",
    "model = get_model(gpu_mode=False)\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=50, verbose=1, restore_best_weights=True)\n",
    "model.fit(X, y, validation_split=0.25, batch_size=128, epochs=500,  callbacks=[early_stop, ckpt], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('./models/ALLFIT_17-0.7645.hdf5')\n",
    "# model = tf.keras.models.load_model('./models/ALLFIT_01-0.4865.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef0c680",
   "metadata": {},
   "source": [
    "## Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fb2e9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "정확도 :0.7980582524271844, seed_num = 0\n",
      "정확도 :0.7974110032362459, seed_num = 5\n",
      "정확도 :0.8077669902912621, seed_num = 10\n",
      "정확도 :0.8032362459546926, seed_num = 15\n",
      "정확도 :0.7896440129449838, seed_num = 20\n",
      "정확도 :0.8122977346278317, seed_num = 25\n",
      "정확도 :0.8181229773462784, seed_num = 30\n",
      "정확도 :0.7870550161812297, seed_num = 35\n",
      "정확도 :0.7902912621359224, seed_num = 40\n",
      "정확도 :0.7915857605177994, seed_num = 45\n",
      "seed = 42의 정확도 df 만들고 평균 확인 : [0.79954693]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./models/ALLFIT1_10-0.7578.hdf5')\n",
    "\n",
    "dic={}\n",
    "for seed in range(0, 50, 5):\n",
    "    random.seed(seed)\n",
    "\n",
    "    x = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "    y = np.load('/project/LSH/y_(7727,1).npy')\n",
    "\n",
    "    idx = list(range(len(x)))\n",
    "    random.shuffle(idx)\n",
    "\n",
    "    i = round(x.shape[0]*0.8)\n",
    "    X_train, y_train = x[idx[:i],:,:], y[idx[:i]]\n",
    "    X_test, y_test = x[idx[i:],:,:], y[idx[i:]]\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    pred[pred>0.5]=1\n",
    "    pred[pred<=0.5]=0\n",
    "    acc = metrics.accuracy_score(y_test, pred)\n",
    "    dic[seed]=acc\n",
    "    print(f'정확도 :{metrics.accuracy_score(y_test, pred)}, seed_num = {seed}')\n",
    "    \n",
    "df = pd.DataFrame.from_dict(dic, orient='index')\n",
    "print(f'seed = {seed_num}의 정확도 df 만들고 평균 확인 : {df.mean().values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ded473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d098fe8a",
   "metadata": {},
   "source": [
    "# Entropy dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5bc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = list(pd.read_csv('/project/LSH/total_data_7727.csv')['ITEMID'].sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "298972aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70c5164a64c49fc89ed2375ad61c714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def entropy(ratio_list):\n",
    "    one_ratio, zero_ratio = ratio_list[0], ratio_list[1] \n",
    "    return - ((one_ratio * (np.log2(one_ratio))) + (zero_ratio * (np.log2(zero_ratio))))\n",
    "\n",
    "X = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "\n",
    "entropy_dict = {}\n",
    "for i in tqdm(range(len(COLS))):\n",
    "    one_ratio = X[:,:,i].sum() / (X.shape[0]*X.shape[1])\n",
    "    zero_ratio = 1 - one_ratio\n",
    "    entropy_num = entropy([one_ratio, zero_ratio])\n",
    "    entropy_dict[COLS[i]] = entropy_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e26f8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.8945475095722644,\n",
       " 50803: 0.033442542699476635,\n",
       " 50804: 0.48487374976369707,\n",
       " 50805: 0.0011721468075888046,\n",
       " 50806: 0.07466452012902748,\n",
       " 50808: 0.3422567690915075,\n",
       " 50809: 0.30631983549914066,\n",
       " 50811: 0.17607571914919884,\n",
       " 50813: 0.3020503924033536,\n",
       " 50814: 0.0013473569150405915,\n",
       " 50818: 0.5905150583063137,\n",
       " 50820: 0.5572216411567222,\n",
       " 50821: 0.6928844725130754,\n",
       " 50822: 0.14596543633389195,\n",
       " 50824: 0.075539219929809,\n",
       " 50852: 0.028723919067390347,\n",
       " 50853: 0.0018581977954349603,\n",
       " 50856: 0.007513335654158085,\n",
       " 50858: nan,\n",
       " 50861: 0.4000048713984587,\n",
       " 50862: 0.4705525702453669,\n",
       " 50863: 0.4083236220110865,\n",
       " 50864: 0.0011721468075888046,\n",
       " 50865: 0.0025135191238101528,\n",
       " 50866: 0.02373004513245036,\n",
       " 50867: 0.11824848231980385,\n",
       " 50868: 0.2937998951444971,\n",
       " 50871: nan,\n",
       " 50872: nan,\n",
       " 50873: 0.0006248993908785407,\n",
       " 50876: 0.00022881204196882252,\n",
       " 50877: 0.00022881204196882252,\n",
       " 50878: 0.45076509428156675,\n",
       " 50881: 0.0018581977954349603,\n",
       " 50882: 0.7677642918971788,\n",
       " 50883: 0.07702015471713725,\n",
       " 50885: 0.31251024860656546,\n",
       " 50889: 0.04128497504640248,\n",
       " 50890: 0.005971387712289948,\n",
       " 50891: 0.003764137875305338,\n",
       " 50892: 0.0026737320789107914,\n",
       " 50893: 0.891331944800732,\n",
       " 50894: nan,\n",
       " 50895: 0.0008117136521852144,\n",
       " 50896: 0.003457538759956708,\n",
       " 50898: 0.00022881204196882252,\n",
       " 50899: 0.0036112831502112816,\n",
       " 50900: 0.0055409111932162875,\n",
       " 50902: 0.7670451352463701,\n",
       " 50905: 0.008739134626849254,\n",
       " 50906: 0.0015198903722008063,\n",
       " 50907: 0.013030749304367146,\n",
       " 50908: 0.07211257334869195,\n",
       " 50909: 0.08533208741139728,\n",
       " 50910: 0.3186383809832029,\n",
       " 50911: 0.13297100527572409,\n",
       " 50912: 0.8482720197728654,\n",
       " 50913: 0.00022881204196882252,\n",
       " 50914: 0.01201506197984063,\n",
       " 50915: 0.015514069617876849,\n",
       " 50916: 0.0004317405758354146,\n",
       " 50917: 0.05170778157294097,\n",
       " 50921: nan,\n",
       " 50922: 0.01526902185977971,\n",
       " 50924: 0.0839765636456385,\n",
       " 50925: nan,\n",
       " 50926: 0.0008117136521852144,\n",
       " 50927: 0.01733105341188729,\n",
       " 50928: nan,\n",
       " 50929: 0.023616852078269038,\n",
       " 50930: 0.007375288486104783,\n",
       " 50931: 0.9998986266406644,\n",
       " 50935: 0.07167023109586314,\n",
       " 50941: nan,\n",
       " 50943: 0.0008117136521852144,\n",
       " 50945: 0.0013473569150405915,\n",
       " 50948: nan,\n",
       " 50949: 0.007925173127203877,\n",
       " 50950: 0.013533529360350637,\n",
       " 50951: 0.007925173127203877,\n",
       " 50952: 0.08201751972168349,\n",
       " 50953: 0.10525705449340983,\n",
       " 50954: 0.36069081092429256,\n",
       " 50956: 0.15160964061368562,\n",
       " 50957: 0.0029904982849564407,\n",
       " 50958: 0.0006248993908785407,\n",
       " 50960: 0.34014652677320795,\n",
       " 50963: 0.07961544446758122,\n",
       " 50964: 0.05093903417644871,\n",
       " 50965: 0.01477683573503425,\n",
       " 50966: 0.008333711668985073,\n",
       " 50967: 0.12329271496242576,\n",
       " 50968: 0.009275133183727548,\n",
       " 50969: 0.012397581750645112,\n",
       " 50970: 0.8034216136523569,\n",
       " 50971: 0.4295070889132209,\n",
       " 50972: 0.000993810084471449,\n",
       " 50973: 0.0020244415239869837,\n",
       " 50974: 0.0036112831502112816,\n",
       " 50976: 0.049005186985993335,\n",
       " 50978: 0.008333711668985073,\n",
       " 50980: 0.0036112831502112816,\n",
       " 50981: 0.0006248993908785407,\n",
       " 50983: 0.5617136646295344,\n",
       " 50986: 0.0222502081230819,\n",
       " 50988: 0.0004317405758354146,\n",
       " 50989: nan,\n",
       " 50990: 0.005251204159487858,\n",
       " 50991: 0.0008117136521852144,\n",
       " 50992: 0.0004317405758354146,\n",
       " 50993: 0.05673551926481965,\n",
       " 50994: 0.008739134626849254,\n",
       " 50995: 0.021560904333509198,\n",
       " 50996: nan,\n",
       " 50997: 0.016123740663885555,\n",
       " 50998: 0.10779725578567226,\n",
       " 51000: 0.045083337190338024,\n",
       " 51001: 0.016849993672881917,\n",
       " 51002: 0.03678799845087898,\n",
       " 51003: 0.3299980220513356,\n",
       " 51005: 0.0008117136521852144,\n",
       " 51006: 0.9809417291532502,\n",
       " 51007: 0.048908035394015774,\n",
       " 51008: 0.024069002763649296,\n",
       " 51009: 0.3187426030552404,\n",
       " 51010: 0.04597231257539709,\n",
       " 51018: 0.0207514166278244,\n",
       " 51070: 0.004664625148799638,\n",
       " 51080: 0.0004317405758354146,\n",
       " 51099: 0.033442542699476635,\n",
       " 51110: 0.0018581977954349603,\n",
       " 51111: 0.0015198903722008063,\n",
       " 51112: 0.0006248993908785407,\n",
       " 51114: 0.0056849321915383375,\n",
       " 51115: 0.0020244415239869837,\n",
       " 51116: 0.027079884792077194,\n",
       " 51117: 0.020402686389944875,\n",
       " 51118: 0.01672934550425164,\n",
       " 51119: 0.0004317405758354146,\n",
       " 51120: 0.020169581448121296,\n",
       " 51121: nan,\n",
       " 51122: 0.0004317405758354146,\n",
       " 51123: 0.003916143392119544,\n",
       " 51124: 0.0015198903722008063,\n",
       " 51125: 0.027520198397221595,\n",
       " 51127: 0.02741025225904116,\n",
       " 51128: 0.02861491359782711,\n",
       " 51130: 0.003302860197138863,\n",
       " 51131: 0.006113851081122915,\n",
       " 51132: 0.004516368037113848,\n",
       " 51136: nan,\n",
       " 51138: 0.00022881204196882252,\n",
       " 51139: 0.0011721468075888046,\n",
       " 51140: 0.000993810084471449,\n",
       " 51143: 0.10748070450622092,\n",
       " 51144: 0.12571531216335016,\n",
       " 51146: 0.010201511878065473,\n",
       " 51148: 0.02597279910093616,\n",
       " 51149: 0.0006248993908785407,\n",
       " 51181: 0.005971387712289948,\n",
       " 51196: 0.036061877053756616,\n",
       " 51199: 0.0015198903722008063,\n",
       " 51200: 0.11763258073520765,\n",
       " 51202: 0.0006248993908785407,\n",
       " 51203: 0.0004317405758354146,\n",
       " 51204: 0.0006248993908785407,\n",
       " 51205: 0.0008117136521852144,\n",
       " 51206: 0.00235197115995385,\n",
       " 51208: 0.0006248993908785407,\n",
       " 51209: 0.0004317405758354146,\n",
       " 51210: nan,\n",
       " 51213: 0.030133547277169798,\n",
       " 51214: 0.13871423605396807,\n",
       " 51218: 0.05934864149797415,\n",
       " 51221: 0.9384783504275023,\n",
       " 51222: 0.9382216973859396,\n",
       " 51223: 0.0004317405758354146,\n",
       " 51224: nan,\n",
       " 51225: 0.00022881204196882252,\n",
       " 51227: 0.0004317405758354146,\n",
       " 51228: 0.000993810084471449,\n",
       " 51232: 0.0026737320789107914,\n",
       " 51237: 0.7423056901229905,\n",
       " 51241: 0.000993810084471449,\n",
       " 51244: 0.5720168579289054,\n",
       " 51245: 0.006113851081122915,\n",
       " 51248: 0.7008944509595152,\n",
       " 51249: 0.5526235792457841,\n",
       " 51250: 0.528750472085986,\n",
       " 51251: 0.14997382506989337,\n",
       " 51253: nan,\n",
       " 51254: 0.170521215038487,\n",
       " 51255: 0.10684677222860434,\n",
       " 51256: 0.576912044091409,\n",
       " 51257: 0.0914440559010062,\n",
       " 51259: 0.015391631507025445,\n",
       " 51263: 0.006255822813845938,\n",
       " 51265: 0.8016527488253533,\n",
       " 51269: 0.024069002763649296,\n",
       " 51270: 0.0004317405758354146,\n",
       " 51271: 0.0015198903722008063,\n",
       " 51272: 0.00022881204196882252,\n",
       " 51273: 0.0008117136521852144,\n",
       " 51274: 0.8731873933256302,\n",
       " 51275: 0.6705406710687218,\n",
       " 51276: 0.000993810084471449,\n",
       " 51277: 0.9193319718674842,\n",
       " 51279: 0.9326676028935987,\n",
       " 51280: nan,\n",
       " 51283: 0.03915270554006575,\n",
       " 51284: 0.01188710314461811,\n",
       " 51288: 0.037098255071061105,\n",
       " 51289: 0.0011721468075888046,\n",
       " 51297: 0.007097992941717205,\n",
       " 51298: 0.0008117136521852144,\n",
       " 51299: 0.0004317405758354146,\n",
       " 51300: 0.004664625148799638,\n",
       " 51301: 0.9334946339796283,\n",
       " 51302: 0.0013473569150405915,\n",
       " 51348: 0.0008117136521852144,\n",
       " 51362: 0.026969584560910326,\n",
       " 51365: 0.00022881204196882252,\n",
       " 51366: nan,\n",
       " 51367: nan,\n",
       " 51368: 0.0008117136521852144,\n",
       " 51369: 0.000993810084471449,\n",
       " 51375: nan,\n",
       " 51376: nan,\n",
       " 51377: 0.0006248993908785407,\n",
       " 51379: 0.0006248993908785407,\n",
       " 51380: 0.00022881204196882252,\n",
       " 51381: 0.0004317405758354146,\n",
       " 51382: 0.0056849321915383375,\n",
       " 51383: 0.0056849321915383375,\n",
       " 51384: 0.0056849321915383375,\n",
       " 51385: 0.0006248993908785407,\n",
       " 51386: 0.0029904982849564407,\n",
       " 51387: 0.0011721468075888046,\n",
       " 51419: 0.010724727953803288,\n",
       " 51422: 0.0026737320789107914,\n",
       " 51427: 0.026638144684833928,\n",
       " 51428: 0.019584606938153528,\n",
       " 51429: 0.006819036254088939,\n",
       " 51430: 0.0004317405758354146,\n",
       " 51431: 0.019701858103070676,\n",
       " 51432: 0.00022881204196882252,\n",
       " 51433: 0.0013473569150405915,\n",
       " 51434: 0.009541273444311058,\n",
       " 51435: 0.00022881204196882252,\n",
       " 51436: 0.028396651501333598,\n",
       " 51437: nan,\n",
       " 51438: 0.006819036254088939,\n",
       " 51439: 0.008873612107579228,\n",
       " 51440: 0.0020244415239869837,\n",
       " 51441: 0.0025135191238101528,\n",
       " 51442: 0.0018581977954349603,\n",
       " 51443: 0.0004317405758354146,\n",
       " 51444: 0.011243788949549608,\n",
       " 51445: 0.004067336636058344,\n",
       " 51446: 0.0341801790218094,\n",
       " 51447: 0.022935477479149008,\n",
       " 51448: 0.021214693060507998,\n",
       " 51449: 0.0006248993908785407,\n",
       " 51450: 0.024519517021440906,\n",
       " 51451: 0.0006248993908785407,\n",
       " 51452: 0.0018581977954349603,\n",
       " 51453: 0.010201511878065473,\n",
       " 51454: 0.0021889853169752985,\n",
       " 51455: 0.03407500806211679,\n",
       " 51456: 0.0004317405758354146,\n",
       " 51457: 0.03354812623200374,\n",
       " 51458: 0.035749730190577124,\n",
       " 51459: nan,\n",
       " 51467: 0.0006248993908785407,\n",
       " 51471: 0.0021889853169752985,\n",
       " 51479: 0.05409314787716287,\n",
       " 51482: 0.1210843338708159,\n",
       " 51491: 0.017929020106525858,\n",
       " 51493: 0.30146097781751824,\n",
       " 51494: 0.0004317405758354146,\n",
       " 51498: 0.028505824533733584,\n",
       " 51507: 0.0036112831502112816,\n",
       " 51514: 0.08550111429401142,\n",
       " 51515: 0.0008117136521852144,\n",
       " 51516: 0.19328082273799707,\n",
       " 51517: 0.0011721468075888046,\n",
       " 51521: nan,\n",
       " 51526: nan,\n",
       " 51529: 0.00022881204196882252,\n",
       " 221214: 0.08894637738652128,\n",
       " 221216: 0.04299346253532324,\n",
       " 221217: 0.05888408739999186,\n",
       " 221219: 0.0006248993908785407,\n",
       " 221223: 0.023956119905207348,\n",
       " 221255: 0.008604329329082328,\n",
       " 223253: 0.02563886891913434,\n",
       " 224263: 0.3087612042936607,\n",
       " 224264: 0.26475647782772527,\n",
       " 224267: 0.04219137786297591,\n",
       " 224268: 0.009938270421707276,\n",
       " 224269: 0.01477683573503425,\n",
       " 224270: 0.09326363039702748,\n",
       " 224272: 0.009541273444311058,\n",
       " 224273: 0.016123740663885555,\n",
       " 224274: 0.15760371758768688,\n",
       " 224275: 0.39665573427340733,\n",
       " 224276: 0.07069458961243691,\n",
       " 224277: 0.3155558131726064,\n",
       " 224385: 0.05485119181962903,\n",
       " 224560: 0.02586158326420461,\n",
       " 224566: 0.010201511878065473,\n",
       " 225199: 0.004067336636058344,\n",
       " 225202: 0.04929637588534824,\n",
       " 225203: 0.004812215061579343,\n",
       " 225204: 0.038128466001297226,\n",
       " 225205: nan,\n",
       " 225315: 0.016123740663885555,\n",
       " 225399: 0.008873612107579228,\n",
       " 225400: 0.046857409946375814,\n",
       " 225401: 0.11344903573037221,\n",
       " 225402: 0.12857321628357785,\n",
       " 225427: 0.006255822813845938,\n",
       " 225428: 0.0004317405758354146,\n",
       " 225429: 0.0011721468075888046,\n",
       " 225430: 0.0056849321915383375,\n",
       " 225432: 0.061474335835937644,\n",
       " 225433: 0.0043674190133042,\n",
       " 225434: 0.002832699146668106,\n",
       " 225436: 0.00235197115995385,\n",
       " 225437: 0.0008117136521852144,\n",
       " 225439: 0.014281772594970073,\n",
       " 225440: 0.008333711668985073,\n",
       " 225441: 0.0362696558543396,\n",
       " 225442: 0.0004317405758354146,\n",
       " 225443: 0.0036112831502112816,\n",
       " 225444: 0.02429446222771068,\n",
       " 225445: 0.008197886956185476,\n",
       " 225446: 0.006819036254088939,\n",
       " 225447: 0.00235197115995385,\n",
       " 225448: 0.007925173127203877,\n",
       " 225449: 0.0004317405758354146,\n",
       " 225450: 0.0029904982849564407,\n",
       " 225451: 0.06312537906580759,\n",
       " 225454: 0.08210295658003686,\n",
       " 225457: 0.015880370165135355,\n",
       " 225459: 0.2555568461159967,\n",
       " 225460: 0.0008117136521852144,\n",
       " 225461: 0.0004317405758354146,\n",
       " 225462: 0.014281772594970073,\n",
       " 225464: 0.006678911116334157,\n",
       " 225465: 0.004067336636058344,\n",
       " 225466: 0.003457538759956708,\n",
       " 225467: nan,\n",
       " 225468: 0.007236843794591585,\n",
       " 225469: 0.025415774495407762,\n",
       " 225470: 0.018878339825633063,\n",
       " 225472: 0.00022881204196882252,\n",
       " 225474: 0.000993810084471449,\n",
       " 225475: 0.0015198903722008063,\n",
       " 225476: 0.0011721468075888046,\n",
       " 225477: nan,\n",
       " 225479: 0.008469190364107194,\n",
       " 225752: 0.35023086881659105,\n",
       " 225789: 0.010201511878065473,\n",
       " 225792: 0.39087943441500717,\n",
       " 225794: 0.050746425317938305,\n",
       " 225802: 0.04734748706074434,\n",
       " 225805: 0.005105483786907171,\n",
       " 225814: 0.031422916911152586,\n",
       " 225816: 0.004664625148799638,\n",
       " 225817: 0.008061709700871028,\n",
       " 225819: 0.00022881204196882252,\n",
       " 225820: 0.0004317405758354146,\n",
       " 225821: 0.0029904982849564407,\n",
       " 225966: 0.06064471881647583,\n",
       " 225967: 0.000993810084471449,\n",
       " 226124: 0.01733105341188729,\n",
       " 226236: 0.0031471982631774637,\n",
       " 226237: 0.0006248993908785407,\n",
       " 226474: 0.0004317405758354146,\n",
       " 226475: 0.0013473569150405915,\n",
       " 226476: 0.0016900836178926667,\n",
       " 226477: 0.0004317405758354146,\n",
       " 227194: 0.08482445752857604,\n",
       " 227550: 0.0015198903722008063,\n",
       " 227551: 0.0018581977954349603,\n",
       " 227711: 0.0020244415239869837,\n",
       " 227712: 0.007236843794591585,\n",
       " 227713: 0.0013473569150405915,\n",
       " 227714: 0.00022881204196882252,\n",
       " 227715: 0.0004317405758354146,\n",
       " 227719: 0.007097992941717205,\n",
       " 228125: 0.02384313421706905,\n",
       " 228126: 0.0013473569150405915,\n",
       " 228127: 0.005971387712289948,\n",
       " 228128: 0.03439031751366816,\n",
       " 228129: 0.050167573713798874,\n",
       " 228130: 0.00235197115995385,\n",
       " 228136: 0.000993810084471449,\n",
       " 228201: 0.0006248993908785407,\n",
       " 228202: 0.0006248993908785407,\n",
       " 228228: 0.0008117136521852144,\n",
       " 228286: 0.0013473569150405915,\n",
       " 2144401: 0.003457538759956708,\n",
       " 2202402: 0.021560904333509198,\n",
       " 2323560: 0.02086741601616271,\n",
       " 2324001: 0.008197886956185476,\n",
       " 2324030: 0.004067336636058344,\n",
       " 2324033: 0.02828739412348354,\n",
       " 2411230: 0.006819036254088939,\n",
       " 2411233: 0.06321678871546424,\n",
       " 2411260: 0.02937621691794291,\n",
       " 2411533: 0.06962629589944178,\n",
       " 2411560: 0.013408138051769401,\n",
       " 2411633: 0.007375288486104783,\n",
       " 2411733: 0.021791129571949588,\n",
       " 2416502: 0.0043674190133042,\n",
       " 2416530: 0.0029904982849564407,\n",
       " 2445385: 0.2146843419986291,\n",
       " 2445485: 0.021099053465250046,\n",
       " 2475977: 0.006397315513110422,\n",
       " 2704001: 0.0006248993908785407,\n",
       " 2714001: 0.000993810084471449,\n",
       " 2717510: 0.0055409111932162875,\n",
       " 2726601: 0.0018581977954349603,\n",
       " 2727125: nan,\n",
       " 2735501: 0.28757774852811707,\n",
       " 2750201: 0.000993810084471449,\n",
       " 2751001: 0.06220946757516854,\n",
       " 2751101: 0.007788270193576144,\n",
       " 2755901: 0.005105483786907171,\n",
       " 2756101: 0.003457538759956708,\n",
       " 2759701: 0.013156750230076474,\n",
       " 2762301: nan,\n",
       " 2821501: 0.31471717503107904,\n",
       " 2831501: 0.2552089733152268,\n",
       " 2841501: 0.0020244415239869837,\n",
       " 2850101: 0.003457538759956708,\n",
       " 2861501: nan,\n",
       " 2871501: 0.04399141266314712,\n",
       " 2897101: nan,\n",
       " 3042950: 0.007375288486104783,\n",
       " 3043730: 0.00653834115123225,\n",
       " 3045051: 0.11282529338368286,\n",
       " 3052411: 0.0004317405758354146,\n",
       " 3056902: 0.00022881204196882252,\n",
       " 3059320: nan,\n",
       " 3080110: 0.000993810084471449,\n",
       " 3083050: 0.011758913557580068,\n",
       " 3085222: 0.0018581977954349603,\n",
       " 3097252: nan,\n",
       " 3161112: 0.0018581977954349603,\n",
       " 3161212: nan,\n",
       " 3161412: 0.00235197115995385,\n",
       " 3173840: nan,\n",
       " 3196401: 0.004664625148799638,\n",
       " 3196501: 0.004959160783258729,\n",
       " 3196601: 0.0011721468075888046,\n",
       " 3196701: nan,\n",
       " 3223011: 0.004959160783258729,\n",
       " 3224011: 0.007375288486104783,\n",
       " 3256016: nan,\n",
       " 3272510: 0.0020244415239869837,\n",
       " 3362312: 0.006678911116334157,\n",
       " 3362412: 0.009007767419682202,\n",
       " 3363112: 0.007650993576660898,\n",
       " 3515405: 0.0025135191238101528,\n",
       " 3517806: 0.018760159851224343,\n",
       " 4001823: 0.004067336636058344,\n",
       " 4003822: 0.045083337190338024,\n",
       " 4003909: 0.01007003061053336,\n",
       " 4005850: 0.013658721797407678,\n",
       " 4006850: 0.025750273361556643,\n",
       " 4012149: 0.002832699146668106,\n",
       " 4012549: 0.0026737320789107914,\n",
       " 4014301: 0.02063529575736423,\n",
       " 4014401: 0.013282545735783408,\n",
       " 4016201: 0.006255822813845938,\n",
       " 4018301: 0.0006248993908785407,\n",
       " 4018401: 0.00022881204196882252,\n",
       " 4024515: 0.00235197115995385,\n",
       " 4024648: nan,\n",
       " 4025001: 0.0056849321915383375,\n",
       " 4025901: 0.015391631507025445,\n",
       " 4026001: 0.04528122699063375,\n",
       " 4026129: 0.03152987006634462,\n",
       " 4026249: 0.0006248993908785407,\n",
       " 4026449: 0.027520198397221595,\n",
       " 4026501: 0.00235197115995385,\n",
       " 4026706: 0.0021889853169752985,\n",
       " 4026948: 0.0018581977954349603,\n",
       " 4029809: 0.020983294704580192,\n",
       " 4035009: 0.0008117136521852144,\n",
       " 4038039: nan,\n",
       " 4050109: nan,\n",
       " 4080085: 0.027630057045912843,\n",
       " 4080285: nan,\n",
       " 4081095: nan,\n",
       " 4082009: 0.0004317405758354146,\n",
       " 4110150: nan,\n",
       " 4196301: nan,\n",
       " 4196401: 0.0036112831502112816,\n",
       " 4200278: 0.13754253181089765,\n",
       " 4200378: 0.030133547277169798,\n",
       " 4201509: 0.0011721468075888046,\n",
       " 4691106: 0.006958726905580202,\n",
       " 4691206: nan,\n",
       " 4692506: 0.01636646074175308,\n",
       " 4694003: 0.012904540678808566,\n",
       " 5010441: 0.0018581977954349603,\n",
       " 5075323: 0.00022881204196882252,\n",
       " 5197067: 0.0018581977954349603,\n",
       " 5230933: 0.00235197115995385,\n",
       " 5381638: nan,\n",
       " 5460923: 0.0004317405758354146,\n",
       " 5534323: 0.0018581977954349603,\n",
       " 5535960: 0.0008117136521852144,\n",
       " 5921823: 0.000993810084471449,\n",
       " 5927029: 0.0015198903722008063,\n",
       " 6003121: 0.02586158326420461,\n",
       " 6003220: 0.0015198903722008063,\n",
       " 6004368: 0.07492721018350611,\n",
       " 6006568: 0.0025135191238101528,\n",
       " 6007228: 0.03248912512393399,\n",
       " 6007428: 0.009007767419682202,\n",
       " 6009268: 0.000993810084471449,\n",
       " 6011728: 0.09400512616495522,\n",
       " 6011731: 0.006113851081122915,\n",
       " 6021468: 0.0004317405758354146,\n",
       " 6022761: 0.015880370165135355,\n",
       " 6027528: 0.0029904982849564407,\n",
       " 6027731: 0.0013473569150405915,\n",
       " 6043268: 0.0004317405758354146,\n",
       " 6046106: 0.0008117136521852144,\n",
       " 6046206: 0.0006248993908785407,\n",
       " 6057143: 0.0018581977954349603,\n",
       " 6057354: 0.0004317405758354146,\n",
       " 6057362: 0.0018581977954349603,\n",
       " 6073128: 0.0025135191238101528,\n",
       " 6073528: 0.03407500806211679,\n",
       " 6074928: 0.0368914793228933,\n",
       " 6074961: 0.008739134626849254,\n",
       " 6092558: 0.00235197115995385,\n",
       " 6093628: 0.0004317405758354146,\n",
       " 6095128: 0.028723919067390347,\n",
       " 6095228: 0.03163674859314429,\n",
       " 6096328: 0.13155991540896808,\n",
       " 6327516: 0.0004317405758354146,\n",
       " 6336603: 0.014281772594970073,\n",
       " 6336703: 0.020169581448121296,\n",
       " 6337666: 0.0008117136521852144,\n",
       " 6340617: nan,\n",
       " 6351659: 0.027520198397221595,\n",
       " 6351903: 0.0018581977954349603,\n",
       " 6351935: 0.01163049033720618,\n",
       " 6351936: 0.010201511878065473,\n",
       " 6353750: 0.15823375769944803,\n",
       " 6354114: 0.00022881204196882252,\n",
       " 6361932: 0.01636646074175308,\n",
       " 6362050: 0.007788270193576144,\n",
       " 6362803: 0.0031471982631774637,\n",
       " 6362835: 0.018523389320193782,\n",
       " 6362836: 0.022020895650446057,\n",
       " 6371350: 0.0004317405758354146,\n",
       " 6382210: 0.02861491359782711,\n",
       " 6382310: 0.02563886891913434,\n",
       " 6384371: 0.000993810084471449,\n",
       " 6394132: 0.0008117136521852144,\n",
       " 6461200: nan,\n",
       " 6473900: 0.22586402955064572,\n",
       " 6482700: nan,\n",
       " 6487600: 0.0006248993908785407,\n",
       " 6489400: nan,\n",
       " 6494300: 0.25758189597062003,\n",
       " 6498100: 0.0006248993908785407,\n",
       " 7313116: nan,\n",
       " 7313505: 0.009141606020069144,\n",
       " 7313705: 0.05990494093798318,\n",
       " 7323001: 0.009806227088853967,\n",
       " 7323211: 0.0025135191238101528,\n",
       " 7323411: nan,\n",
       " 7336203: 0.0043674190133042,\n",
       " 7336344: 0.0004317405758354146,\n",
       " 7336721: 0.049780810649916796,\n",
       " 7351920: 0.0011721468075888046,\n",
       " 7365021: 0.007788270193576144,\n",
       " 7401020: nan,\n",
       " 7411719: 0.000993810084471449,\n",
       " 7413920: 0.027849513494273097,\n",
       " 7414020: 0.020983294704580192,\n",
       " 7414120: 0.017211015928639335,\n",
       " 7440701: 0.02861491359782711,\n",
       " 7447120: 0.0015198903722008063,\n",
       " 7489020: 0.007375288486104783,\n",
       " 7489120: nan,\n",
       " 7489220: 0.006958726905580202,\n",
       " 7506111: 0.0008117136521852144,\n",
       " 7521006: 0.0008117136521852144,\n",
       " 8001901: nan,\n",
       " 8002707: 0.012142792874481605,\n",
       " 8005910: nan,\n",
       " 8021201: nan,\n",
       " 8029501: 0.02259333814436313,\n",
       " 8030302: nan,\n",
       " 8039004: 0.0016900836178926667,\n",
       " 8058107: 0.08844451584490787,\n",
       " 8058113: 0.032701493105695034,\n",
       " 8070101: 0.0043674190133042,\n",
       " 8070102: 0.0018581977954349603,\n",
       " 8070401: 0.005828419423229215,\n",
       " 8070402: 0.007236843794591585,\n",
       " 8072801: nan,\n",
       " 8077102: 0.007788270193576144,\n",
       " 8078102: 0.01046365392452512,\n",
       " 8081401: 0.04812921224699993,\n",
       " 8083303: 0.04047559028218637,\n",
       " 8083701: 0.007925173127203877,\n",
       " 8083703: 0.007375288486104783,\n",
       " 8083721: 0.0006248993908785407,\n",
       " 8084199: 0.6106978804023773,\n",
       " 8092303: 0.20897383857192864,\n",
       " 8092355: 0.5917874651676952,\n",
       " 8092681: nan,\n",
       " 8103003: nan,\n",
       " 8103006: 0.011372931102785921,\n",
       " 8103007: nan,\n",
       " 8103105: 0.005828419423229215,\n",
       " 8103110: 0.005971387712289948,\n",
       " 8104005: 0.0021889853169752985,\n",
       " 8104110: 0.01033271832282573,\n",
       " 8117901: 0.0004317405758354146,\n",
       " 8121801: 0.014157544003226063,\n",
       " 8412501: 0.0018581977954349603,\n",
       " 8412601: nan,\n",
       " 8418806: 0.12458155819533992,\n",
       " 8451001: nan,\n",
       " 8499002: 0.0056849321915383375,\n",
       " 8499020: 0.004959160783258729,\n",
       " 8536002: 0.0008117136521852144,\n",
       " 9000402: 0.012270298572028895,\n",
       " 9000602: 0.004067336636058344,\n",
       " 9001104: 0.03658084968400997,\n",
       " 9001201: 0.008739134626849254,\n",
       " 9003101: 0.00022881204196882252,\n",
       " 9003928: nan,\n",
       " 9004902: 0.0031471982631774637,\n",
       " 9005002: nan,\n",
       " 9005605: 0.003457538759956708,\n",
       " 9006406: 0.0018581977954349603,\n",
       " 9006412: 0.0018581977954349603,\n",
       " 9007301: 0.0031471982631774637,\n",
       " 9011312: 0.03884604952794588,\n",
       " 9011319: 0.07087223525874846,\n",
       " 9012101: 0.0025135191238101528,\n",
       " 9014102: 0.009806227088853967,\n",
       " 9017103: 0.016002137426160433,\n",
       " 9019009: 0.02508041168446148,\n",
       " 9019016: 0.05541809207618793,\n",
       " 9023301: nan,\n",
       " 9035201: nan,\n",
       " 9037301: nan,\n",
       " 9043624: nan,\n",
       " 9045003: 0.0011721468075888046,\n",
       " 9074635: nan,\n",
       " 9075801: 0.0025135191238101528,\n",
       " 9076004: 0.000993810084471449,\n",
       " 9076502: 0.009007767419682202,\n",
       " 9079601: 0.0004317405758354146,\n",
       " 9082501: 0.020983294704580192,\n",
       " 9090020: 0.10581425491663732,\n",
       " 9307301: nan,\n",
       " 9311602: 0.003457538759956708,\n",
       " 9337501: 0.03227646892601313,\n",
       " 9337502: 0.028068624805747965,\n",
       " 9338101: 0.0018581977954349603,\n",
       " 9338102: 0.0018581977954349603,\n",
       " 9338201: 0.000993810084471449,\n",
       " 9338202: 0.000993810084471449,\n",
       " 9338901: 0.07078342674501209,\n",
       " 9347501: 0.0004317405758354146,\n",
       " 9361702: 0.005105483786907171,\n",
       " 9361703: 0.00022881204196882252,\n",
       " 9361803: 0.007788270193576144,\n",
       " 9376103: 0.0011721468075888046,\n",
       " 9379401: 0.0036112831502112816,\n",
       " 9454101: 0.008061709700871028,\n",
       " 9454102: 0.0026737320789107914,\n",
       " 9454401: 0.013156750230076474,\n",
       " 9454402: 0.010201511878065473,\n",
       " 9513502: 0.03884604952794588,\n",
       " 9513503: 0.032913575064034105,\n",
       " 9513601: 0.0025135191238101528,\n",
       " 9514001: 0.10835055879148475,\n",
       " 9521301: nan,\n",
       " 9752901: 0.0008117136521852144,\n",
       " 9766304: 0.000993810084471449,\n",
       " 9770901: 0.000993810084471449,\n",
       " 13010111: nan,\n",
       " 13010201: 0.0004317405758354146,\n",
       " 13110679: 0.0004317405758354146,\n",
       " 13258691: 0.0013473569150405915,\n",
       " 13530117: nan,\n",
       " 13700112: 0.000993810084471449,\n",
       " 13746686: 0.0011721468075888046,\n",
       " 13830304: 0.11298132632703585,\n",
       " 15050301: 0.0018581977954349603,\n",
       " 15050541: 0.00022881204196882252,\n",
       " 15050842: 0.006397315513110422,\n",
       " 15054841: 0.003764137875305338,\n",
       " 15301020: 0.00022881204196882252,\n",
       " 15301238: nan,\n",
       " 15302020: 0.000993810084471449,\n",
       " 15309520: 0.006819036254088939,\n",
       " 15321230: nan,\n",
       " 15321530: 0.0004317405758354146,\n",
       " 15322022: 0.003457538759956708,\n",
       " 15347530: 0.0004317405758354146,\n",
       " 15356302: 0.0004317405758354146,\n",
       " 15710328: 0.022821540039943237,\n",
       " 15733912: 0.0006248993908785407,\n",
       " 15740320: nan,\n",
       " 15740420: 0.004516368037113848,\n",
       " 15740520: 0.01526902185977971,\n",
       " 15797020: 0.0029904982849564407,\n",
       " 15798120: nan,\n",
       " 19481604: 0.009541273444311058,\n",
       " 23024001: 0.011243788949549608,\n",
       " 23031204: 0.11438271189318949,\n",
       " 23031207: 0.0031471982631774637,\n",
       " 23031304: 0.0018581977954349603,\n",
       " 23050601: 0.2083425197074058,\n",
       " 23218105: 0.004067336636058344,\n",
       " 23791560: 0.0013473569150405915,\n",
       " 23866505: 0.021791129571949588,\n",
       " 23916330: 0.000993810084471449,\n",
       " 23916332: 0.004812215061579343,\n",
       " 23917705: 0.06531027829817564,\n",
       " 23918703: 0.012651490950831006,\n",
       " 24030506: 0.003916143392119544,\n",
       " 24059010: 0.0004317405758354146,\n",
       " 24059120: nan,\n",
       " 24107501: 0.0004317405758354146,\n",
       " 24115510: 0.11126136584186364,\n",
       " 24120006: 0.0018581977954349603,\n",
       " 24120020: 0.0021889853169752985,\n",
       " 24120301: 0.025192296104610306,\n",
       " 24123305: 0.0004317405758354146,\n",
       " 24135202: 0.0015198903722008063,\n",
       " 24135301: 0.0036112831502112816,\n",
       " 24156210: 0.011114400878537989,\n",
       " 24159601: 0.005251204159487858,\n",
       " 24230110: 0.0018581977954349603,\n",
       " 24279150: 0.0018581977954349603,\n",
       " 24414260: 0.002832699146668106,\n",
       " 24515010: 0.0006248993908785407,\n",
       " 24540134: 0.15387561798057403,\n",
       " 25103134: 0.010594322549157367,\n",
       " 25145134: 0.003764137875305338,\n",
       " 25146134: nan,\n",
       " 25152031: 0.004812215061579343,\n",
       " 25152531: 0.005971387712289948,\n",
       " 25171001: 0.006255822813845938,\n",
       " 25273234: 0.0015198903722008063,\n",
       " 25274234: 0.0025135191238101528,\n",
       " 25540134: 0.053522959527236465,\n",
       " 26285548: 0.012651490950831006,\n",
       " 26285570: 0.004959160783258729,\n",
       " 26286148: 0.0008117136521852144,\n",
       " 26556161: 0.004516368037113848,\n",
       " 26815120: 0.0004317405758354146,\n",
       " 26848858: 0.0004317405758354146,\n",
       " 26851248: 0.00653834115123225,\n",
       " 26851348: 0.01201506197984063,\n",
       " 26851448: nan,\n",
       " 26855236: 0.006958726905580202,\n",
       " 26855463: 0.02418178335298506,\n",
       " 26858188: nan,\n",
       " 26858231: nan,\n",
       " 26884148: 0.003764137875305338,\n",
       " 26885148: 0.006113851081122915,\n",
       " 26886148: 0.0013473569150405915,\n",
       " 29152544: 0.00022881204196882252,\n",
       " 29152611: 0.022364695956343796,\n",
       " 29152722: 0.022020895650446057,\n",
       " 29152725: 0.011372931102785921,\n",
       " 29315818: 0.004067336636058344,\n",
       " 29315913: 0.005396340731895448,\n",
       " 29316020: 0.0013473569150405915,\n",
       " 29320613: 0.0018581977954349603,\n",
       " 29320713: 0.0026737320789107914,\n",
       " 29321013: 0.019935974292515923,\n",
       " 29321121: 0.03439031751366816,\n",
       " 29321213: 0.0055409111932162875,\n",
       " 29321548: 0.013408138051769401,\n",
       " 29485121: 0.0006248993908785407,\n",
       " 29485220: nan,\n",
       " 29607531: 0.0021889853169752985,\n",
       " 29608031: 0.014529670767072803,\n",
       " 29608621: 0.00022881204196882252,\n",
       " 29609039: 0.004959160783258729,\n",
       " 29657126: 0.0015198903722008063,\n",
       " 31067364: 0.004959160783258729,\n",
       " 31425064: 0.0018581977954349603,\n",
       " 31670972: 0.0761499327746554,\n",
       " 31782463: 0.004812215061579343,\n",
       " 31867412: 0.018523389320193782,\n",
       " 31890163: 0.0006248993908785407,\n",
       " 32102301: nan,\n",
       " 32121001: 0.010984763613882906,\n",
       " 32121201: 0.013156750230076474,\n",
       " 32160280: 0.01007003061053336,\n",
       " 32192482: 0.0018581977954349603,\n",
       " 32192824: 0.0018581977954349603,\n",
       " 32421011: 0.0020244415239869837,\n",
       " 32449201: nan,\n",
       " 34050510: 0.0004317405758354146,\n",
       " 34051425: 0.003764137875305338,\n",
       " 34051525: 0.004217751461779114,\n",
       " 34051625: 0.0031471982631774637,\n",
       " 34051725: 0.0018581977954349603,\n",
       " 34120081: 0.24490876146273388,\n",
       " 34180080: 0.1818333219524888,\n",
       " 34210001: 0.0031471982631774637,\n",
       " 34549006: 0.0006248993908785407,\n",
       " 34700480: 0.0026737320789107914,\n",
       " 37024130: 0.0011721468075888046,\n",
       " 37043001: 0.002832699146668106,\n",
       " 39001910: 0.0021889853169752985,\n",
       " 39007810: 0.02719009587190399,\n",
       " 39010411: nan,\n",
       " 39022110: 0.0013473569150405915,\n",
       " 39022211: 0.0018581977954349603,\n",
       " 39022311: 0.002832699146668106,\n",
       " 44101101: 0.007925173127203877,\n",
       " 44101201: 0.0018581977954349603,\n",
       " 44101710: 0.007788270193576144,\n",
       " 44101725: 0.0029904982849564407,\n",
       " 44182110: 0.004516368037113848,\n",
       " 44182210: 0.01636646074175308,\n",
       " 44182310: 0.008469190364107194,\n",
       " 44182512: 0.004217751461779114,\n",
       " 44182610: 0.0056849321915383375,\n",
       " 44182712: 0.004067336636058344,\n",
       " 44502210: 0.005828419423229215,\n",
       " 45006601: 0.16046689029284275,\n",
       " 45006701: 0.1623426883267038,\n",
       " 45006801: 0.20865827411066212,\n",
       " 45009569: 0.004812215061579343,\n",
       " 45025301: 0.0006248993908785407,\n",
       " 45025501: 0.23351267563195285,\n",
       " 45025549: nan,\n",
       " 45034360: 0.0025135191238101528,\n",
       " 45044804: 0.012904540678808566,\n",
       " 45050130: 0.2890572910881583,\n",
       " 45051372: 0.01733105341188729,\n",
       " 45063965: 0.015146239090038035,\n",
       " 45064165: 0.021676074700183495,\n",
       " 45064265: 0.0026737320789107914,\n",
       " 45065910: 0.032913575064034105,\n",
       " 45081015: 0.0026737320789107914,\n",
       " 45152010: 0.2501407955735258,\n",
       " 45152510: 0.27336265909479884,\n",
       " 46037406: nan,\n",
       " 46047081: 0.0004317405758354146,\n",
       " 46047181: 0.000993810084471449,\n",
       " 46074905: 0.0011721468075888046,\n",
       " 46086699: 0.0004317405758354146,\n",
       " 46086799: 0.006113851081122915,\n",
       " 46086881: nan,\n",
       " 46087221: nan,\n",
       " 46087293: 0.0031471982631774637,\n",
       " 46110081: 0.0004317405758354146,\n",
       " 46110281: 0.0015198903722008063,\n",
       " 48102003: 0.005828419423229215,\n",
       " 48104013: 0.008604329329082328,\n",
       " 48105013: 0.01007003061053336,\n",
       " 48106003: 0.0029904982849564407,\n",
       " 48107013: 0.012270298572028895,\n",
       " 48108003: 0.004217751461779114,\n",
       " 48109013: 0.006819036254088939,\n",
       " 48113013: 0.008739134626849254,\n",
       " 48114013: 0.000993810084471449,\n",
       " 48580601: 0.0013473569150405915,\n",
       " 48580701: 0.0013473569150405915,\n",
       " 49001383: 0.000993810084471449,\n",
       " 49001483: 0.010594322549157367,\n",
       " 49002428: 0.004812215061579343,\n",
       " 49011528: 0.0004317405758354146,\n",
       " 49052083: 0.0004317405758354146,\n",
       " 49053028: 0.006958726905580202,\n",
       " 49155066: 0.0021889853169752985,\n",
       " 49156066: 0.0015198903722008063,\n",
       " 49275041: 0.005105483786907171,\n",
       " 49276041: 0.002832699146668106,\n",
       " 49277041: 0.0018581977954349603,\n",
       " 49317030: 0.006678911116334157,\n",
       " 49318030: 0.0362696558543396,\n",
       " 49319028: 0.027079884792077194,\n",
       " 49342041: 0.013156750230076474,\n",
       " 49343041: 0.02741025225904116,\n",
       " 49343526: 0.018167191811796747,\n",
       " 49343626: 0.012270298572028895,\n",
       " 49345019: 0.0004317405758354146,\n",
       " 49350079: 0.0020244415239869837,\n",
       " 49392083: 0.005396340731895448,\n",
       " 49396041: 0.0016900836178926667,\n",
       " 49397041: 0.0021889853169752985,\n",
       " 49398041: 0.002832699146668106,\n",
       " 49399041: 0.00235197115995385,\n",
       " 49490041: 0.05560675121770865,\n",
       " 49491041: 0.033336889516024364,\n",
       " 51002121: 0.006397315513110422,\n",
       " 51842530: 0.0008117136521852144,\n",
       " 51845030: 0.0008117136521852144,\n",
       " 52010590: 0.059719648273408706,\n",
       " 52010630: 0.00235197115995385,\n",
       " 52010730: 0.029267705349282396,\n",
       " 52044115: 0.0248563498752919,\n",
       " 52045015: 0.0011721468075888046,\n",
       " 52045016: 0.0006248993908785407,\n",
       " 52046002: 0.0018581977954349603,\n",
       " 52073110: 0.017450941284878542,\n",
       " 52073112: 0.019935974292515923,\n",
       " 53767006: 0.0018581977954349603,\n",
       " 53768002: 0.014529670767072803,\n",
       " 54000725: 0.011372931102785921,\n",
       " 54001720: 0.10827156691520232,\n",
       " 54001820: 0.16710221679301548,\n",
       " 54001920: 0.021906069655885593,\n",
       " 54002025: 0.0016900836178926667,\n",
       " 54002821: 0.0018581977954349603,\n",
       " 54007928: 0.0011721468075888046,\n",
       " 54008620: 0.019818980260536495,\n",
       " 54008826: 0.03354812623200374,\n",
       " 54009720: 0.009275133183727548,\n",
       " 54009920: 0.003764137875305338,\n",
       " 54010625: 0.00022881204196882252,\n",
       " 54011625: 0.0011721468075888046,\n",
       " 54017613: 0.0004317405758354146,\n",
       " 54022463: 0.047249564554186205,\n",
       " 54023524: 0.042291822348543576,\n",
       " 54023624: 0.005396340731895448,\n",
       " 54023755: 0.028396651501333598,\n",
       " 54121842: 0.009141606020069144,\n",
       " 54301050: 0.14983129472631107,\n",
       " 54315440: nan,\n",
       " 54317763: nan,\n",
       " 54319446: 0.00022881204196882252,\n",
       " 54327099: 0.08986442895548782,\n",
       " 54329450: nan,\n",
       " 54335050: 0.0026737320789107914,\n",
       " 54350049: 0.016487580357292097,\n",
       " 54350547: 0.0021889853169752985,\n",
       " 54352763: 0.0018581977954349603,\n",
       " 54353244: 0.0021889853169752985,\n",
       " 54354563: 0.030994352019589177,\n",
       " 54355563: 0.0036112831502112816,\n",
       " 54355663: 0.007788270193576144,\n",
       " 54368263: 0.006819036254088939,\n",
       " 54368663: 0.0004317405758354146,\n",
       " 54372144: nan,\n",
       " 54375150: 0.01709082763052959,\n",
       " 54376950: 0.011758913557580068,\n",
       " 54380563: 0.006113851081122915,\n",
       " 54408425: 0.021214693060507998,\n",
       " 54412125: 0.0011721468075888046,\n",
       " 54412925: 0.000993810084471449,\n",
       " 54413025: 0.0018581977954349603,\n",
       " 54418425: 0.023956119905207348,\n",
       " 54422125: 0.0013473569150405915,\n",
       " 54449625: 0.007236843794591585,\n",
       " 54453825: 0.0013473569150405915,\n",
       " 54465025: 0.07237764253097652,\n",
       " 54485906: 0.0011721468075888046,\n",
       " 54800204: 0.006958726905580202,\n",
       " 54808425: 0.006678911116334157,\n",
       " 54811616: 0.02563886891913434,\n",
       " 54812025: 0.02586158326420461,\n",
       " 54813916: 0.00022881204196882252,\n",
       " 54814622: 0.03428528209092153,\n",
       " 54815624: 0.029701266602213797,\n",
       " 54816303: 0.0004317405758354146,\n",
       " 54817525: 0.02619495044845775,\n",
       " 54817625: 0.03152987006634462,\n",
       " 54817925: 0.009541273444311058,\n",
       " 54818125: 0.006113851081122915,\n",
       " 54819216: 0.007236843794591585,\n",
       " 54819316: 0.0011721468075888046,\n",
       " 54828104: 0.023049306751707035,\n",
       " 54829725: 0.11453809994422057,\n",
       " 54829816: 0.012397581750645112,\n",
       " 54829925: 0.10068703619426414,\n",
       " 54830125: 0.041587592999678055,\n",
       " 54834225: 0.018286064590929033,\n",
       " 54834425: 0.016487580357292097,\n",
       " 54834525: 0.0207514166278244,\n",
       " 54834625: 0.0008117136521852144,\n",
       " 54839224: 0.1332674627877792,\n",
       " 54839424: 0.010201511878065473,\n",
       " 54848204: 0.0026737320789107914,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f6bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123ab26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69de2bbd",
   "metadata": {},
   "source": [
    "# 2. {E(0to1) - E(1to0)} * Entropy ^ Lambda (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f0d3520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c69868be684c09847a975971ae8da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.device('/device:GPU:1'): \n",
    "    model = tf.keras.models.load_model('./models/ALLFIT_17-0.7645.hdf5')\n",
    "    result = []\n",
    "    for i in tqdm(range(X.shape[2])):\n",
    "        save_cols = X[:,:,i].copy()\n",
    "        #-----zero2one-----\n",
    "        X[:,:,i] = 1\n",
    "        pred1 = model.predict(X, batch_size=10000, workers=-1, use_multiprocessing=True)\n",
    "        mean_pred1 = np.mean(pred1)\n",
    "        #-----one2zero-----\n",
    "        X[:,:,i] = 0\n",
    "        pred2 = model.predict(X, batch_size=10000, workers=-1, use_multiprocessing=True)\n",
    "        mean_pred2 = np.mean(pred2)\n",
    "\n",
    "        result.append({'feature' : str(COLS[i]), 'zero2one' : mean_pred1, 'one2zero' : mean_pred2,\n",
    "                       'lambda0' : mean_pred1 - mean_pred2, 'lambda1' : (mean_pred1 - mean_pred2) * entropy_dict[COLS[i]]})\n",
    "\n",
    "        #값 복원\n",
    "        X[:,:,i] = save_cols\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63196c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>one2zero</th>\n",
       "      <th>zero2one</th>\n",
       "      <th>lambda0</th>\n",
       "      <th>lambda1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.789104</td>\n",
       "      <td>0.754038</td>\n",
       "      <td>0.035065</td>\n",
       "      <td>0.031368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  one2zero  zero2one   lambda0   lambda1\n",
       "0       0  0.789104  0.754038  0.035065  0.031368"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(result).sort_values('lambda0', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbbda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "489923d3",
   "metadata": {},
   "source": [
    "# 3. Feature Importance - Sequential Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd0d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f91ce699e74d44848414bb5ea0117e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Top10 Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fd90cf21194dfbb7722ea81ade3859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero, one Predict:   0%|          | 0/4068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd0241575b9489493611d99f97a1fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero, one Predict:   0%|          | 0/4068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-da8fd387904b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#-----one2zero-----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mmean_pred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1697\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1155\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m                **kwargs):\n\u001b[1;32m    246\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    249\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[0;32m-> 1430\u001b[0;31m   return convert_to_tensor_v2(\n\u001b[0m\u001b[1;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   \u001b[0;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m   return convert_to_tensor(\n\u001b[0m\u001b[1;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#-----FI 측정 시작-----\n",
    "with tf.device('/device:GPU:0'): \n",
    "    #-----모델 로드-----\n",
    "    model = tf.keras.models.load_model('./models/ALLFIT_17-0.7645.hdf5')\n",
    "    top10 = []\n",
    "    top10_result = []\n",
    "    for n in tqdm(range(1), desc=\"Top10 Iteration\"):\n",
    "        result = []\n",
    "        if len(top10) > 1:\n",
    "            print('####################  top10 :', [COLS[i] for i in top10])\n",
    "            for i in tqdm(range(X.shape[2]), desc=\"zero, one Predict\"):\n",
    "                save_cols = X[:,:,top10].copy()\n",
    "                #-----zero2one-----\n",
    "                X[:,:,top10] = 1\n",
    "                pred1 = model.predict(X, batch_size=10000, workers=-1, use_multiprocessing=True)\n",
    "                mean_pred1 = np.mean(pred1)\n",
    "                #-----one2zero-----\n",
    "                X[:,:,top10] = 0\n",
    "                pred2 = model.predict(X, batch_size=10000, workers=-1, use_multiprocessing=True)\n",
    "                mean_pred2 = np.mean(pred2)\n",
    "                result.append({'feature_index' : i\n",
    "                            'lambda0' : mean_pred1 - mean_pred2, 'lambda1' : (mean_pred1 - mean_pred2) * entropy_dict[COLS[i]]})\n",
    "            df = pd.DataFrame(result).sort_values('lambda0', ascending=False)\n",
    "            print(df)\n",
    "            top10.append(df.feature_index[n])\n",
    "        else:\n",
    "            for i in tqdm(range(X.shape[2]), desc=\"zero, one Predict\"):\n",
    "                save_cols = X[:,:,i].copy()\n",
    "                #-----zero2one-----\n",
    "                X[:,:,i] = 1\n",
    "                pred1 = model.predict(X, batch_size=10000, workers=-1, use_multiprocessing=True)\n",
    "                mean_pred1 = np.mean(pred1)\n",
    "                #-----one2zero-----\n",
    "                X[:,:,i] = 0\n",
    "                pred2 = model.predict(X, batch_size=10000, workers=-1, use_multiprocessing=True)\n",
    "                mean_pred2 = np.mean(pred2)\n",
    "\n",
    "                result.append({'feature_index' : i, 'zero2one' : mean_pred1, 'one2zero' : mean_pred2,\n",
    "                            'lambda0' : mean_pred1 - mean_pred2, 'lambda1' : (mean_pred1 - mean_pred2) * entropy_dict[COLS[i]]})\n",
    "\n",
    "            df = pd.DataFrame(result).sort_values('lambda0', ascending=False)\n",
    "            top10.append(df.feature_index[0])\n",
    "\n",
    "\n",
    "#     #-----df저장-----\n",
    "#     df.to_csv('./data/Method_Sequential_FI.csv', index=False)\n",
    "#     #-----top10 feature만 저장-----\n",
    "#     top10_list = list(map(lambda x:str(COLS[x]), top10))\n",
    "#     print(top10_list)\n",
    "#     with open('./data/Sequential_top10.txt', \"w\", encoding=\"UTF-8\") as file:\n",
    "#         json.dump(top10_list, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d053b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_index</th>\n",
       "      <th>one2zero</th>\n",
       "      <th>zero2one</th>\n",
       "      <th>lambda0</th>\n",
       "      <th>lambda1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>2611</td>\n",
       "      <td>0.507915</td>\n",
       "      <td>0.507874</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>3832</td>\n",
       "      <td>0.507955</td>\n",
       "      <td>0.507916</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>2388</td>\n",
       "      <td>0.507905</td>\n",
       "      <td>0.507868</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>3456</td>\n",
       "      <td>0.507941</td>\n",
       "      <td>0.507908</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>619</td>\n",
       "      <td>0.507929</td>\n",
       "      <td>0.507899</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.508144</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.508113</td>\n",
       "      <td>0.508157</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.508107</td>\n",
       "      <td>0.508152</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.508046</td>\n",
       "      <td>0.508094</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>206</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>0.507986</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4068 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_index  one2zero  zero2one   lambda0   lambda1\n",
       "2611           2611  0.507915  0.507874  0.000041  0.000023\n",
       "3832           3832  0.507955  0.507916  0.000040  0.000032\n",
       "2388           2388  0.507905  0.507868  0.000037  0.000022\n",
       "3456           3456  0.507941  0.507908  0.000034  0.000018\n",
       "619             619  0.507929  0.507899  0.000030  0.000018\n",
       "...             ...       ...       ...       ...       ...\n",
       "12               12  0.508100  0.508144 -0.000043 -0.000030\n",
       "10               10  0.508113  0.508157 -0.000044 -0.000026\n",
       "11               11  0.508107  0.508152 -0.000045 -0.000025\n",
       "56               56  0.508046  0.508094 -0.000048 -0.000041\n",
       "206             206  0.507932  0.507986 -0.000054 -0.000049\n",
       "\n",
       "[4068 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f5957",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0178e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try1 = pd.read_csv('./data/Method_eel_FI.csv').sort_values('feature').reset_index(drop=True)\n",
    "try1['lambda0'] = try1['zero2one'] - try1['one2zero'] \n",
    "try1['lambda1'] = try1['lambda0'] * list(entropy_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f51629e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>one2zero</th>\n",
       "      <th>zero2one</th>\n",
       "      <th>lambda0</th>\n",
       "      <th>lambda1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>62856024641</td>\n",
       "      <td>0.345287</td>\n",
       "      <td>0.905962</td>\n",
       "      <td>0.560675</td>\n",
       "      <td>0.006665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>67159011203</td>\n",
       "      <td>0.382268</td>\n",
       "      <td>0.931299</td>\n",
       "      <td>0.549031</td>\n",
       "      <td>0.004575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>68084024711</td>\n",
       "      <td>0.376804</td>\n",
       "      <td>0.923922</td>\n",
       "      <td>0.547118</td>\n",
       "      <td>0.011353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>67286081203</td>\n",
       "      <td>0.380851</td>\n",
       "      <td>0.921074</td>\n",
       "      <td>0.540222</td>\n",
       "      <td>0.014689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>59011010325</td>\n",
       "      <td>0.376409</td>\n",
       "      <td>0.915132</td>\n",
       "      <td>0.538723</td>\n",
       "      <td>0.004490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836</th>\n",
       "      <td>63323026965</td>\n",
       "      <td>0.446835</td>\n",
       "      <td>0.130318</td>\n",
       "      <td>-0.316518</td>\n",
       "      <td>-0.069818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>63459010001</td>\n",
       "      <td>0.425043</td>\n",
       "      <td>0.104555</td>\n",
       "      <td>-0.320488</td>\n",
       "      <td>-0.000376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>904564061</td>\n",
       "      <td>0.463617</td>\n",
       "      <td>0.138598</td>\n",
       "      <td>-0.325019</td>\n",
       "      <td>-0.001322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>63323065102</td>\n",
       "      <td>0.428467</td>\n",
       "      <td>0.096872</td>\n",
       "      <td>-0.331595</td>\n",
       "      <td>-0.001789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>63323038810</td>\n",
       "      <td>0.433617</td>\n",
       "      <td>0.064301</td>\n",
       "      <td>-0.369315</td>\n",
       "      <td>-0.005594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4068 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  one2zero  zero2one   lambda0   lambda1\n",
       "3789  62856024641  0.345287  0.905962  0.560675  0.006665\n",
       "3982  67159011203  0.382268  0.931299  0.549031  0.004575\n",
       "4020  68084024711  0.376804  0.923922  0.547118  0.011353\n",
       "3987  67286081203  0.380851  0.921074  0.540222  0.014689\n",
       "3586  59011010325  0.376409  0.915132  0.538723  0.004490\n",
       "...           ...       ...       ...       ...       ...\n",
       "3836  63323026965  0.446835  0.130318 -0.316518 -0.069818\n",
       "3882  63459010001  0.425043  0.104555 -0.320488 -0.000376\n",
       "2648    904564061  0.463617  0.138598 -0.325019 -0.001322\n",
       "3871  63323065102  0.428467  0.096872 -0.331595 -0.001789\n",
       "3852  63323038810  0.433617  0.064301 -0.369315 -0.005594\n",
       "\n",
       "[4068 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try1.sort_values('lambda0',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88855abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c94b72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>one2zero</th>\n",
       "      <th>zero2one</th>\n",
       "      <th>lambda0</th>\n",
       "      <th>lambda1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>51277</td>\n",
       "      <td>0.51719</td>\n",
       "      <td>0.347983</td>\n",
       "      <td>0.169207</td>\n",
       "      <td>0.155557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  one2zero  zero2one   lambda0   lambda1\n",
       "3972    51277   0.51719  0.347983  0.169207  0.155557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try2 = pd.read_csv('./data/Method_eel_FI_3852.csv').sort_values('lambda0', ascending=False)\n",
    "try2[try2['feature']==63323038810]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd458efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>one2zero</th>\n",
       "      <th>zero2one</th>\n",
       "      <th>lambda0</th>\n",
       "      <th>lambda1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>63323011710</td>\n",
       "      <td>0.344205</td>\n",
       "      <td>0.070149</td>\n",
       "      <td>0.274056</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>63323012250</td>\n",
       "      <td>0.344214</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.282888</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>63323010510</td>\n",
       "      <td>0.344280</td>\n",
       "      <td>0.106804</td>\n",
       "      <td>0.237477</td>\n",
       "      <td>0.011034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>63323014210</td>\n",
       "      <td>0.344323</td>\n",
       "      <td>0.063023</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.000428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>63323013210</td>\n",
       "      <td>0.344324</td>\n",
       "      <td>0.072559</td>\n",
       "      <td>0.271765</td>\n",
       "      <td>0.000319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>50866</td>\n",
       "      <td>0.655095</td>\n",
       "      <td>0.478588</td>\n",
       "      <td>0.176506</td>\n",
       "      <td>0.004189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>50867</td>\n",
       "      <td>0.656576</td>\n",
       "      <td>0.307940</td>\n",
       "      <td>0.348636</td>\n",
       "      <td>0.041226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>50878</td>\n",
       "      <td>0.658026</td>\n",
       "      <td>0.381286</td>\n",
       "      <td>0.276740</td>\n",
       "      <td>0.124745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>50881</td>\n",
       "      <td>0.658030</td>\n",
       "      <td>0.398346</td>\n",
       "      <td>0.259684</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>50861</td>\n",
       "      <td>0.658792</td>\n",
       "      <td>0.293830</td>\n",
       "      <td>0.364961</td>\n",
       "      <td>0.145986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4068 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  one2zero  zero2one   lambda0   lambda1\n",
       "3341  63323011710  0.344205  0.070149  0.274056  0.001073\n",
       "3163  63323012250  0.344214  0.061326  0.282888  0.000230\n",
       "3703  63323010510  0.344280  0.106804  0.237477  0.011034\n",
       "3203  63323014210  0.344323  0.063023  0.281300  0.000428\n",
       "3373  63323013210  0.344324  0.072559  0.271765  0.000319\n",
       "...           ...       ...       ...       ...       ...\n",
       "3954        50866  0.655095  0.478588  0.176506  0.004189\n",
       "1085        50867  0.656576  0.307940  0.348636  0.041226\n",
       "3278        50878  0.658026  0.381286  0.276740  0.124745\n",
       "3514        50881  0.658030  0.398346  0.259684  0.000483\n",
       "595         50861  0.658792  0.293830  0.364961  0.145986\n",
       "\n",
       "[4068 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try2.sort_values('one2zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcf52cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLS.index(50856)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59100ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5eeff4",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10545136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(top10_list):\n",
    "    #-----데이터 로드-----\n",
    "    X = np.load('/project/LSH/x_(7727,10,4068).npy')\n",
    "    y = np.load('/project/LSH/y_(7727,1).npy')\n",
    "    #-----컬럼이름 로드-----\n",
    "    COLS = list(pd.read_csv('/project/LSH/total_data_7727.csv')['ITEMID'].sort_values().unique())\n",
    "    #-----사망/퇴원 환자 분리-----\n",
    "    d_index, s_index = np.where(y==1)[0], np.where(y==0)[0]\n",
    "    d_X, s_X = X[d_index], X[s_index]\n",
    "    result_d, result_s = [], []\n",
    "    #-----사망/퇴원 환자별 item 1의 합계 구하기-----\n",
    "    #day - 10일\n",
    "    for d in range(10):\n",
    "        #4068 - ITEM\n",
    "        for f in range(d_X.shape[-1]):\n",
    "            d_sum = d_X[:,d,f].sum()/d_X.shape[0]\n",
    "            s_sum = s_X[:,d,f].sum()/s_X.shape[0]\n",
    "            result_d.append({'cols':COLS[f], 'day':10-d,'per':d_sum})\n",
    "            result_s.append({'cols':COLS[f], 'day':10-d,'per':s_sum})\n",
    "    #-----최종 합계 df-----\n",
    "    d_df = pd.DataFrame(result_d).sort_values(['cols','day']).reset_index(drop=True)\n",
    "    s_df = pd.DataFrame(result_s).sort_values(['cols','day']).reset_index(drop=True)\n",
    "    #-----Visualization-----\n",
    "    plt.figure(figsize = (13,12), dpi=150)\n",
    "    for i, f in enumerate(top10_list):\n",
    "        plt.subplot(4,3,1+i)\n",
    "        plt.title(f)\n",
    "        ax = sns.lineplot(data = d_df[d_df['cols']==int(f)], x = 'day', y='per', label='사망')\n",
    "        ax = sns.lineplot(data = s_df[s_df['cols']==int(f)], x = 'day', y='per', label='퇴원')\n",
    "        ax.invert_xaxis()\n",
    "        ax.legend(loc='upper left')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58dd61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e2805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1667f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "184.926px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
